{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WDL 2021 - Stage 3: Predicting the demand for shared bicycles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOA - CAROL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notas: A nossa solução inicial assumia que era possível fazer o tracing das bicicletas, mas após ser fornecido o dicionário, percebemos que tal não era possível dado que \"The same bike can have multiple tags over the year\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing\n",
    "\n",
    "Explicar o que foi limpo, etc. e justificar. (RESUMO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and customize seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from math import floor\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "fig_dim = (15,12)\n",
    "\n",
    "base = '/home/ana/Downloads/'\n",
    "#base = r'C:\\Users\\Carolina Alves\\OneDrive - Universidade de Aveiro\\WDL competition\\Stage 3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliar functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot function\n",
    "def bar_plot(_x, _y, _data):\n",
    "    fig, axs = plt.subplots(figsize=fig_dim)\n",
    "    axs = sns.barplot(x=_x, y=_y, data=_data)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.draw()\n",
    "    \n",
    "# performs the encoding of categorical features to ordinal numbers\n",
    "def encode_categorical_features(_df, features):\n",
    "    df = _df.copy()\n",
    "    enc = OrdinalEncoder()\n",
    "    df[features] = enc.fit_transform(df[features]).astype(int)\n",
    "    \n",
    "    return enc, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening and saving given dataset - we converted the file from csv to binary in order to read it faster\n",
    "#df = pd.read_csv(path+'/'+\"bike_loans.csv\", sep=\";\")\n",
    "#pickle.dump(df, open(path+'/'+\"df_raw\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening data\n",
    "df = pickle.load(open(path+'/'+\"df_raw\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data types, uniques and NaN information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each column, check type of data, number of unique values and the presence of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eda_to_df(df):\n",
    "    header=\"+\" + (\"-\"*85) + \"+\"\n",
    "    form = \"|{:^25s}|{:^10s}|{:^10s}|{:^10s}|{:^15s}|{:^10s}|\"\n",
    "    print(header)\n",
    "    print(form.format(\"Column\", \"Type\", \"Uniques\", \"NaN?\", \"Number of NaN\" ,\"%NaN\"))\n",
    "    print(header)\n",
    "    for col in df.columns:\n",
    "        print(form.format(str(col), # Column\n",
    "                          str(df[col].dtypes), # Type\n",
    "                          str(len(df[col].unique())), # Uniques\n",
    "                          str(df[col].isnull().values.any()), # NaN?\n",
    "                          str(df[col].isnull().sum()), # Number of NaNs\n",
    "                          str(round(((df[col].isnull().sum())/len(df[col]))*100,5)) )) # %NaN\n",
    "    print(header)\n",
    "\n",
    "eda_to_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ANÁLISE TABELA]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importe e DescripcionImporte têm apenas um valor que se repete em todas as linhas e por isso são removidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Importe', 'DescripcionImporte'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Looking for relations between NaN values in columns Id_Aparcamiento_Destino, Posicion_Destino and operario***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_apar_destino = df[df['Id_Aparcamiento_Destino'].isnull()]\n",
    "print('Number of NaN in Id_Aparcamiento_Destino column: ', len(df_apar_destino))\n",
    "\n",
    "df_pos_destino = df[df['Posicion_Destino'].isnull()]\n",
    "print('\\nNumber of NaN in Posicion_Destino column: ', len(df_pos_destino))\n",
    "\n",
    "df_pos_apar = df[(df['Id_Aparcamiento_Destino'].isnull()) & (df['Posicion_Destino'].isnull())]\n",
    "print('\\nNumber of rows when Id_Aparcamiento_Destino and Posicion_Destino are NaN: ', len(df_pos_apar))\n",
    "\n",
    "df_pos_apar_ope = df[(df['Id_Aparcamiento_Destino'].isnull()) & (df['Posicion_Destino'].isnull()) & (df['operario']=='monitor')]\n",
    "print('\\nNumber of rows when Id_Aparcamiento_Destino and Posicion_Origen are NaN and the operario is monitor: ', len(df_pos_apar_ope))\n",
    "\n",
    "df_pos_apar_ope_null = df[(df['Id_Aparcamiento_Destino'].isnull()) & (df['Posicion_Destino'].isnull()) & (df['operario'].isnull())]\n",
    "print('\\nNumber of rows when Id_Aparcamiento_Destino, Posicion_Origen and operario are NaN: ', len(df_pos_apar_ope_null))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_apar_destino.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando o Id_Aparcamiento_Destino tem valores NaN não conseguimos saber qual a estação de destino. Por essa razão esses dados não nos interessam pois não trazem informação ao modelo que irá ser construído. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Id_Aparcamiento_Destino'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Calculating the usage duration of bicycles***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duration (prestamo, devolucion):\n",
    "\n",
    "    start = datetime.datetime.strptime(prestamo[0:19], \"%Y-%m-%d %H:%M:%S\")\n",
    "    end = datetime.datetime.strptime(devolucion[0:19], \"%Y-%m-%d %H:%M:%S\")\n",
    "    time = end - start\n",
    "    seconds = time.total_seconds()\n",
    "\n",
    "    return seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Duration'] = df.apply(lambda x: duration(x['Fecha_Prestamo'],x['Fecha_Devolucion']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Removing data when the usage duration of bicycles is negative (which means that the day of prestamo is after the day of devolucion)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Duration']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Removing data before 2016 and after 2020***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data_between(df, year_init, year_end):\n",
    "\n",
    "    df_list = []\n",
    "    increment = 1000000\n",
    "    for i in range(0,15000000, increment):\n",
    "        df_ = df.iloc[i:i+increment,:]\n",
    "        #creating column with the year of prestamo and devolucion\n",
    "        df_['Year_Prestamo'] = df_.apply(lambda x: year_(x['Fecha_Prestamo']),axis=1) \n",
    "        df_['Year_Devolucion'] = df_.apply(lambda x: year_(x['Fecha_Devolucion']),axis=1) \n",
    "        #removing years before year_init and after year_end\n",
    "        df_= df_[(df_['Year_Prestamo']>=year_init) & (df_['Year_Prestamo']<year_end)]\n",
    "        df_= df_[(df_['Year_Devolucion']>=year_init) & (df_['Year_Devolucion']<year_end)]\n",
    "        df_list.append(df_)\n",
    "\n",
    "    #creating cleaned dataframe\n",
    "    df_new_ = pd.DataFrame()\n",
    "    df_clean_ = df_new_.append(other=df_list,ignore_index=True)\n",
    "\n",
    "    return df_clean_\n",
    "\n",
    "df_clean_ = select_data_between(df, 2016, 2020)\n",
    "\n",
    "#confirming\n",
    "print(df_clean_['Year_Prestamo'].unique())\n",
    "print(df_clean_['Year_Devolucion'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Encoding categorical features***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of NaN in column Id_Tag_Bicicleta: ', df_clean_['Id_Tag_Bicicleta'].isnull().sum())\n",
    "print('\\nNumber of NaN in column operario: ', df_clean_['operario'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operario_ = df_clean_['operario'].fillna(value='other')\n",
    "df_clean_['operario'] = operario_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc, df_clean_ = encode_categorical_features(df_clean_, [\"Id_Tag_Bicicleta\", \"operario\"])\n",
    "df_clean_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving cleaned dataframe\n",
    "pickle.dump(df_clean_, open(path+'/'+\"df_clean\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loans_clean(base):\n",
    "\n",
    "    df = pd.read_pickle(base + 'df_clean')[[\"Id_Historico_Prestamo\", \"Id_Usuario\", \"Id_Tag_Bicicleta\", \\\n",
    "            \"Fecha_Prestamo\", \"Fecha_Devolucion\", \"Id_Aparcamiento_Origen\", \"Posicion_Origen\", \\\n",
    "            \"Id_Aparcamiento_Destino\", \"Posicion_Destino\"]]\n",
    "    \n",
    "    # Rename columns\n",
    "    # The same bike can have multiple tags over the year.\n",
    "    df = df.rename(columns={\n",
    "        \"Id_Historico_Prestamo\": \"Loan ID\",\n",
    "        \"Id_Usuario\": \"User ID\",\n",
    "        \"Id_Tag_Bicicleta\": \"Tag ID\",\n",
    "        \"Fecha_Prestamo\": \"Start loan\",\n",
    "        \"Fecha_Devolucion\": \"End loan\",\n",
    "        \"Id_Aparcamiento_Origen\": \"ID start station\",\n",
    "        \"Posicion_Origen\": \"Position start\",\n",
    "        \"Id_Aparcamiento_Destino\": \"ID end station\",\n",
    "        \"Posicion_Destino\": \"Position end\"\n",
    "    })\n",
    "    \n",
    "    df[\"Start loan\"] = pd.to_datetime(df[\"Start loan\"], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    df[\"End loan\"] = pd.to_datetime(df[\"End loan\"], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    # Auxiliar fields\n",
    "    df['year_prestamo'] = df[\"Start loan\"].dt.year\n",
    "    df['month_prestamo'] = df[\"Start loan\"].dt.month\n",
    "    df['day_prestamo'] = df[\"Start loan\"].dt.day\n",
    "    df['hour_prestamo'] = df[\"Start loan\"].dt.hour\n",
    "    \n",
    "    df['year_devolucion'] = df[\"End loan\"].dt.year\n",
    "    df['month_devolucion'] = df[\"End loan\"].dt.month\n",
    "    df['day_devolucion'] = df[\"End loan\"].dt.day\n",
    "    df['hour_devolucion'] = df[\"End loan\"].dt.hour\n",
    "    \n",
    "    df['duration_hour'] = (df[\"End loan\"]-df[\"Start loan\"]).dt.total_seconds().div(60*60)\n",
    "    df['Fecha_Prestamo_htruncate'] = df['Start loan'].dt.floor('h')\n",
    "        \n",
    "    return df\n",
    "\n",
    "df = get_loans_clean(base)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stations information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stations location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info = pd.read_csv(\"station_location_with_id.csv\", sep=';')[['ID','numSlots','isManual']]\n",
    "df_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAY</th>\n",
       "      <th>DATE</th>\n",
       "      <th>HOLIDAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Friday</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>New Year's Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Monday</td>\n",
       "      <td>2016-01-11</td>\n",
       "      <td>Epiphany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Monday</td>\n",
       "      <td>2016-03-21</td>\n",
       "      <td>Saint Joseph's Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thursday</td>\n",
       "      <td>2016-03-24</td>\n",
       "      <td>Maundy Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Friday</td>\n",
       "      <td>2016-03-25</td>\n",
       "      <td>Good Friday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DAY        DATE             HOLIDAY\n",
       "0    Friday  2016-01-01      New Year's Day\n",
       "1    Monday  2016-01-11            Epiphany\n",
       "2    Monday  2016-03-21  Saint Joseph's Day\n",
       "3  Thursday  2016-03-24     Maundy Thursday\n",
       "4    Friday  2016-03-25         Good Friday"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_holidays():\n",
    "    holidays16 = pd.read_excel('holidays_columbia.xlsx', sheet_name='2016')\n",
    "    holidays17 = pd.read_excel('holidays_columbia.xlsx', sheet_name='2017')\n",
    "    holidays18 = pd.read_excel('holidays_columbia.xlsx', sheet_name='2018')\n",
    "    holidays19 = pd.read_excel('holidays_columbia.xlsx', sheet_name='2019')\n",
    "    holidays = pd.concat([holidays16, holidays17, holidays18, holidays19])\n",
    "    return holidays\n",
    "\n",
    "holidays = get_holidays()\n",
    "holidays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e1b3c3c9d97d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DATE'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Fecha_Prestamo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdfDATE_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DATE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mholidays_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfDATE_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mholidays\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'DATE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df['DATE'] = df['Fecha_Prestamo'].dt.date.astype(str)\n",
    "dfDATE_count = df.groupby('DATE').size().reset_index(name='count')\n",
    "holidays_count = pd.merge(dfDATE_count,holidays,on='DATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=fig_dim)\n",
    "sns.lineplot(data=dfDATE_count, x=\"DATE\", y=\"count\",  ax=ax, label=\"Number of loads per day\")\n",
    "sns.regplot(data=holidays_count, x=\"DATE\", y=\"count\", fit_reg=False, \n",
    "            scatter_kws={\"color\":\"black\",\"alpha\":1,\"s\":50}, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution of loans, returns, stations... over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quais são as horas/dias da semana/meses mais comuns para usar uma bicicleta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prestamo_over_time(_df):\n",
    "    df = _df.copy()\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=fig_dim)\n",
    "    axis = [ axs[0, 0], axs[0, 1], axs[1, 0], axs[1, 1]]\n",
    "    \n",
    "    time_units = [\"year\", \"month\", \"day\", \"hour\"]\n",
    "    \n",
    "    for i in range(len(time_units)):\n",
    "        time_unit = time_units[i]\n",
    "        df_time = df.groupby(time_unit+'_prestamo').size().reset_index(name='count')\n",
    "\n",
    "        axis[i].plot(df_time[time_unit+\"_prestamo\"], df_time[\"count\"])\n",
    "        axis[i].set_title(time_unit+\"_prestamo\")\n",
    "\n",
    "    plt.plot()\n",
    "\n",
    "plot_prestamo_over_time(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quais é que são os tempos médios de empréstimo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDuration=df.groupby('duration_hour').size().reset_index(name='count')\n",
    "ax=dfDuration.plot(x=\"duration_hour\", y=\"count\", figsize=(16,9))\n",
    "ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quais as origens/destinos mais comuns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOrigin=df.groupby('ID start station').size().reset_index(name='count').sort_values(by=['count'])    \n",
    "dfOrigin[\"ID start station\"] = dfOrigin[\"ID start station\"].astype(str)\n",
    "bar_plot(\"ID start station\", \"count\", dfOrigin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDestination=df.groupby('ID end station').size().reset_index(name='count').sort_values(by=['count'])    \n",
    "dfDestination[\"ID end station\"] = dfDestination[\"ID end station\"].astype(str)\n",
    "bar_plot(\"ID end station\", \"count\", dfDestination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quais os trajetos mais comuns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_popular_routes(df):\n",
    "    \n",
    "    df_routes = df.groupby(['ID start station', 'ID end station']).size().reset_index(name='count')\n",
    "\n",
    "    fig = plt.figure(figsize=fig_dim)\n",
    "    ax = Axes3D(fig)\n",
    "\n",
    "    ax.plot_trisurf(df_routes[\"ID start station\"], df_routes[\"ID end station\"], \\\n",
    "                    df_routes[\"count\"], cmap=cm.jet, linewidth=0.2)\n",
    "    ax.set_zlabel('Count')\n",
    "    ax.set_xlabel('Origin')\n",
    "    ax.set_ylabel('Destination')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "plot_popular_routes(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# justificar porque é que só usamos os desta coluna\n",
    "unique_parks = df['ID start station'].unique()\n",
    "unique_parks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cada nó, as datas/dias e a contagem do check-in (empréstimos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOrigemDayCount=df.groupby([\"ID start station\", pd.Grouper(key=\"Fecha_Prestamo_htruncate\", freq=\"D\")]) \\\n",
    "                        .size().reset_index(name='Count')\n",
    "dfOrigemDayCount.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO\n",
    "# gráfico\n",
    "# após a previsão encontrar o dia com mais nós em que foi realizada a previsão\n",
    "\n",
    "# nº de nós associados ao dia\n",
    "dfNodesDay = dfOrigemDayCount.groupby(\"Fecha_Prestamo_htruncate\").size().reset_index() \\\n",
    "    .rename(columns={0:'count'}).sort_values(by=['count'])\n",
    "\n",
    "dfNodesDay.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_stations_per_day(_df):\n",
    "    df = _df.copy().sort_values(by=['Fecha_Prestamo_htruncate'])\n",
    "    fig = plt.figure(figsize=fig_dim)\n",
    "    plt.plot(df[\"Fecha_Prestamo_htruncate\"], df[\"count\"])\n",
    "\n",
    "active_stations_per_day(dfNodesDay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isto dá-nos a data com mais nós associados mas não garante que os 85 nós tem 4 entradas antes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loans_per_day_per_station(_df, station):\n",
    "    df = _df.copy()\n",
    "    df = df[df[\"ID start station\"]==station]\n",
    "    df = df[[\"Fecha_Prestamo_htruncate\", \"Count\"]]\n",
    "    \n",
    "    fig = plt.figure(figsize=fig_dim)\n",
    "    plt.plot(df[\"Fecha_Prestamo_htruncate\"], df[\"Count\"])\n",
    "\n",
    "loans_per_day_per_station(dfOrigemDayCount, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_per_day_per_station(dfOrigemDayCount, 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loans per station per hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loans_or_returns_per_station_per_time(_df, field_date, field_station, field_position, time):\n",
    "    df = _df.copy()\n",
    "    df = df[[field_date, field_station, field_position]]\n",
    "    df[field_position] = [1]*len(df.index)\n",
    "    #df[field_date] = df[field_date].dt.round(time)\n",
    "    \n",
    "    if time == 'd':\n",
    "        df[field_date] = df[field_date].dt.date\n",
    "    elif time == 'h':\n",
    "        df[field_date] = df[field_date].dt.floor('h')\n",
    "\n",
    "    df = df.rename(columns={field_date:'Date', field_station:'Station'})\n",
    "    df = df.groupby(['Station', 'Date']).size().to_frame()\n",
    "    \n",
    "    if(field_date=='Start loan'):\n",
    "        df = df.rename(columns={0:'N Loans'})\n",
    "    else:\n",
    "        df = df.rename(columns={0:'N Returns'})\n",
    "        \n",
    "    return df\n",
    "\n",
    "def get_loans_and_returns_info_per_time(_df, time):\n",
    "    loans = get_loans_or_returns_per_station_per_time(_df, \\\n",
    "                    'Start loan', 'ID start station', 'Position start', time)\n",
    "        \n",
    "    returns = get_loans_or_returns_per_station_per_time(_df, \\\n",
    "                    'End loan', 'ID end station', 'Position end', time)\n",
    "        \n",
    "    df = pd.concat([loans, returns], axis=1)\n",
    "    df = df.fillna(0)\n",
    "    df['balance_per_' + time] = df['N Returns'] - df['N Loans']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_and_returns = get_loans_and_returns_info_per_time(df, \"h\")\n",
    "loans_and_returns.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a station, this function calculates the number of bikes through time using the loans and the returns\n",
    "def get_balance_for_station(_df, station):\n",
    "    df = _df.copy()\n",
    "    df = df.reset_index()\n",
    "    df = df.loc[df['Station']==station]\n",
    "    df['N Loans cumsum'] = df['N Loans'].cumsum()\n",
    "    df['N Returns cumsum'] = df['N Returns'].cumsum() # returns\n",
    "    df['Balance'] = df['N Returns cumsum'] - df['N Loans cumsum']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_balance_over_multiple_stations(loans_and_returns, stations):\n",
    "    \n",
    "    fig, axs = plt.subplots(4, 2, figsize=(15,24))\n",
    "    axis = [ axs[0, 0], axs[0, 1], axs[1, 0], axs[1, 1],\n",
    "             axs[2, 0], axs[2, 1], axs[3, 0], axs[3, 1]]\n",
    "    colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n",
    "    \n",
    "    for i in range(8):\n",
    "        station = stations[i]\n",
    "    \n",
    "        balance_st = get_balance_for_station(loans_and_returns, station)\n",
    "        \n",
    "        axis[i].plot(balance_st['Date'], balance_st['Balance'], colors[i])\n",
    "        axis[i].set_title('Station ' + str(station))\n",
    "        axis[i].tick_params(labelrotation=30)\n",
    "\n",
    "    plt.plot()\n",
    "\n",
    "plot_balance_over_multiple_stations(loans_and_returns, stations=[1, 70, 54, 45, 12, 70, 2, 21])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dizer o que significa as subidas, descidas e estabilizações - recebm mais do que emprestam, emprestam mais do que receber e recebem e emprestam na mesma porporção (respetivamente)\n",
    "Através do balanço não dá para perceber qual o número de bicicletas inicial da estação, porque esistem várias trocas manuais que não são reportadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO\n",
    "# gráfico\n",
    "# após a previsão encontrar o dia com mais nós em que foi realizada a previsão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obter o número inicial de bicicletas por estação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver o primeiro mês em que a estação abre, e fazer a contagens por aí\n",
    "\n",
    "def get_init_number_of_bikes_per_station(_df):\n",
    "    df = _df.copy()\n",
    "    df = df[[\"Tag ID\", \"Start loan\", \"ID start station\"]]\n",
    "    df[\"Start loan\"] = pd.to_datetime(df[\"Start loan\"], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    df = df.sort_values(by=\"Start loan\")\n",
    "    # for each tag get first station\n",
    "    df = df.groupby(\"Tag ID\").first() \\\n",
    "            .reset_index() \\\n",
    "            [[\"Tag ID\", \"ID start station\"]]\n",
    "    # for each station get initial number of bikes \n",
    "    df = df.groupby(\"ID start station\").count()\n",
    "    df = df.rename(columns={'Tag ID':'Count'})\n",
    "    df = df.sort_values(by=['Count'])\n",
    "    df = df.reset_index()\n",
    "    df[\"ID start station\"] = df[\"ID start station\"].astype(str)\n",
    "    return df\n",
    "    \n",
    "init_n_bikes = get_init_number_of_bikes_per_station(df)\n",
    "init_n_bikes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=fig_dim)\n",
    "axs = sns.barplot(x=\"ID start station\", y=\"Count\", data=init_n_bikes)\n",
    "plt.xticks(rotation=90)\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above represents the number of bikes that appear for the first time in each station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A estação 9 está associada ao aparecimento pela primeira vez de 400 bicicletas (do ID de uma bicicleta)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[BRUNO] Qual é a localização das estações com mais de 100 contagens? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificar quando as estações automáticas cheias. -> ver para cada estação qual o nº máximo de posições diferentes e depois tentar perceber qnd é que elas estão cheias (elas estão cheias se num dado intervalo de tempo todas as positions estão ocupadas) ---> taxa de ocupação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há posições que nunca são ocupadas!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver a taxa de ocupação a cada hora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver a capacidade de armazenamento das estações automáticas. Quais são as estações automáticas com maior capacidade de armazenamento? Quais as com menor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capacity_per_station(_df):\n",
    "    df = _df.copy()\n",
    "    df[\"ID\"] = df[\"ID\"].astype(str)\n",
    "    df = df.sort_values(by=['numSlots'])\n",
    "    \n",
    "    fig, axs = plt.subplots(figsize=(15,12))\n",
    "    axs = sns.barplot(x=\"ID\", y=\"numSlots\", hue=\"isManual\", data=df)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.draw()\n",
    "    \n",
    "capacity_per_station(df_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possivel vizualizar que as estações manuais existem em menor quantidade que as automáticas..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting the ?field?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOrigemDayCount.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_and_returns_per_day = get_loans_and_returns_info_per_time(df, \"d\")\n",
    "loans_and_returns_per_day.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver parâmetros para melhorar, \n",
    "# se o count_before for 8 ele dá 8 entradas antes\n",
    "# offset é o nº de dias antes do atual\n",
    "# distância entre cada uma das entradas\n",
    "# distance=1, count_before=3, offset=0 ---> considera os 3 dias antes\n",
    "# distance=7, count_before=4, offset=7 ---> 4 entradas com a diferença de 7 dias entre elas \n",
    "#                                              e começo a contar na semana anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliar functions\n",
    "def generate_features(df, cols, distance=1, count_before=3, offset=0):\n",
    "    # Filter rows in which we cannot trace back history\n",
    "    periodDf = df[df.index-(count_before*distance+offset)>=0].copy()\n",
    "    # Number of removed elements is the offset for the new df\n",
    "    n_removed=df.shape[0]-periodDf.shape[0]\n",
    "    # Add new features based on traced history\n",
    "    for col in cols:\n",
    "        for i in range(0, count_before):\n",
    "            diff=(i+1)*distance+offset\n",
    "            periodDf[col+\"_minus_\" + str(diff)]=periodDf.apply(lambda row: df.iloc[int(row.name)-diff][col], \\\n",
    "                                                               axis=1) \n",
    "        \n",
    "    return periodDf\n",
    "\n",
    "def prepare_inputs(X_train, X_test):\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe.fit(X_train)\n",
    "    X_train_enc = ohe.transform(X_train)\n",
    "    X_test_enc = ohe.transform(X_test)\n",
    "    return X_train_enc, X_test_enc\n",
    "\n",
    "\n",
    "def split_df(df, train_size=0.8):\n",
    "    train_size = floor(df.shape[0]*train_size)\n",
    "\n",
    "    train = df[:train_size]\n",
    "    test = df[train_size:]\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "def process_df(train, test, dfFeatures):\n",
    "    trainX = train[dfFeatures.columns[4:]]\n",
    "    trainX = trainX.to_numpy().reshape(trainX.shape[0], trainX.shape[1], 1)\n",
    "    trainY = train[[\"Count\"]]\n",
    "    trainY = trainY.to_numpy().reshape(trainY.shape[0])\n",
    "    testX = test[dfFeatures.columns[4:]]\n",
    "    testX = testX.to_numpy().reshape(testX.shape[0], testX.shape[1], 1)\n",
    "    testY = test[[\"Count\"]]\n",
    "    testY = testY.to_numpy().reshape(testY.shape[0])\n",
    "    \n",
    "    \n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    verbose, epochs, batch_size = 0, 10, 32\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[0], trainX.shape[1], trainX.shape[2]\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=6, kernel_size=2, activation='relu', input_shape=(n_features, n_outputs)))\n",
    "    model.add(Conv1D(filters=6, kernel_size=2, activation='relu'))\n",
    "    #model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    #model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(1, activation='relu')) \n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mean_squared_error'])\n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return model, accuracy\n",
    "\n",
    "\n",
    "def run_experiment(trainX, trainy, testX, testy, repeats=10):\n",
    "    min_score=-1\n",
    "    min_model=None\n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        print(\"Train #{}....\".format(r))\n",
    "        model, score = evaluate_model(trainX, trainy, testX, testy)\n",
    "        if min_score==-1 or score<min_score:\n",
    "            min_score=score\n",
    "            min_model=model\n",
    "        scores.append(score)\n",
    "    return min_model, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_forecasting_all_nodes(_df, parks):\n",
    "    \n",
    "    df = _df.copy()\n",
    "\n",
    "    allNodesDf=pd.DataFrame()\n",
    "    # CREATE MODEL PER NODE AND PREDICT TEST SET BASED ON EACH INDIVIDUAL MODEL\n",
    "    for park in parks:\n",
    "        # FILTERING ENTRIES TO PARK ID\n",
    "        print (\"Filtering entries by park id {}...\".format(park))\n",
    "        dfParkId = df[df[\"ID start station\"]==park]\n",
    "        dfParkId=dfParkId.reset_index()\n",
    "\n",
    "        # GENERATE FEATURES (4 ENTRIES 7 DAYS APPART FROM EACH OTHER FROM THE PREVIOUS 4 WEEKS)\n",
    "        print (\"Generating park {} features...\".format(park))\n",
    "        dfFeatures=generate_features(dfParkId, [\"Count\"], distance=7, count_before=4, offset=7)\n",
    "        dfFeatures=dfFeatures.reset_index(drop=True)\n",
    "\n",
    "        # SPLITTING DF INTO TRAIN AND TEST SETS\n",
    "        print (\"Splitting train/test dataset for park id {}...\".format(park))\n",
    "        train, test = split_df(dfFeatures)\n",
    "        trainX, trainy, testX, testy = process_df(train, test, dfFeatures)\n",
    "\n",
    "        # TRAIN CNN\n",
    "        print (\"Traning model for park id {}...\".format(park))\n",
    "        model, scores = run_experiment(trainX, trainy, testX, testy, repeats=3)\n",
    "\n",
    "        # PREDICT TEST SET SAMPLES\n",
    "        print (\"Predicting samples for park id {}...\".format(park))\n",
    "        predict = model.predict(testX)\n",
    "        dfPredict=pd.DataFrame(data=predict.flatten()[:], columns=[\"predict\"])\n",
    "        truth=pd.DataFrame(data=testy[:], columns=[\"truth\"])\n",
    "        testDf=pd.concat([dfPredict, truth, test.reset_index()[[\"ID start station\", \"Fecha_Prestamo_htruncate\"]]], \\\n",
    "                         axis=1)\n",
    "        testDf.set_index([\"ID start station\", \"Fecha_Prestamo_htruncate\"], inplace = True)\n",
    "        allNodesDf=pd.concat([allNodesDf, testDf])\n",
    "        print(allNodesDf)\n",
    "        \n",
    "    return allNodesDf\n",
    "\n",
    "# perform_forecasting_all_nodes(dfOrigemDayCount, unique_parks)\n",
    "allNodesDf = perform_forecasting_all_nodes(dfOrigemDayCount, [8, 56])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rms = np.sqrt(mean_squared_error(allNodesDf[\"truth\"], allNodesDf[\"predict\"]))\n",
    "print (\"OVERALL ROOT MEAN SQUARED ERROR >> \" + str(rms))\n",
    "\n",
    "for node in unique_parks:\n",
    "    print (\" ------------------ NODE {} TRUTH VS PREDICTED CHECK INS ------------------ \".format(node))\n",
    "    ax=allNodesDf.loc[node].plot(figsize=fig_dim, title='Predicted vs ground truth number of infractions over time (for test set)')\n",
    "\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Number of infractions\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "## Scalability and Impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
